\chapter{Model selection and evaluation}
\section{Cross-validation: evaluating estimator performance}
Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called \textbf{overfitting}. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a \textbf{test set} \texttt{X\_test}, \texttt{y\_test}. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques in \nameref{Tuning the hyper-parameters of an estimator}.
\begin{figure}[H]
\centering
\includegraphics{img/grid_search_workflow.png}
\caption{\url{https://scikit-learn.org/stable/_images/grid_search_workflow.png}}
\label{s}
\end{figure}


\section{Tuning the hyper-parameters of an estimator\label{Tuning the hyper-parameters of an estimator}}